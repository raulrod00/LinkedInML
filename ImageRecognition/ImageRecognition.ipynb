{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169e0798-d165-4f45-a928-f48637071d5b",
   "metadata": {},
   "source": [
    "# Image Recognition with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e38cd5-fb6c-40d6-94a6-b855a1590eee",
   "metadata": {},
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed7dbc-fe48-4efd-abe3-b2c5f1a409bb",
   "metadata": {},
   "source": [
    "### View Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f4ce1c-5008-46b7-b075-c00428eb8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af078cf-a7ac-41d3-9600-dc77f8af0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of names for each CIFAR10 class\n",
    "cifar10_class_names = {\n",
    "    0: \"Plane\",\n",
    "    1: \"Car\",\n",
    "    2: \"Bird\",\n",
    "    3: \"Cat\",\n",
    "    4: \"Deer\",\n",
    "    5: \"Dog\",\n",
    "    6: \"Frog\",\n",
    "    7: \"Horse\",\n",
    "    8: \"Boat\",\n",
    "    9: \"Truck\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd38136f-03b8-4f8b-a6e0-6535ad7967c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdXElEQVR4nO2dfaxlZ1XGn7X32eeee+6905nplOl02tKhVGBQafFSilZSQbA0MS2JQaoh/aNxQKlKgokVIy0GEzACQWMgg60UgpQKRapWpTZoQ6qFWxym09Z+MrUznc5H78zcz/Oxz17+cfaYO/Vd636dj9L3+SU399y9zrv3Ou/e6+xz3+estURVQQh5+ZMM2wFCyGBgsBMSCQx2QiKBwU5IJDDYCYkEBjshkcBgJ6chIp8XkT9axfOvEJED/fSJ9IbKsB0gg0dE9gPYCqADoA3gfgAfUNVnVfUDw/SN9A/e2ePll1V1HMA2AIcB/MVyA0SEN4cfYxjskaOqDQBfB7ATAETkiyLy8fLxFSJyQER+X0SeB/DXIjJaPue4iDwC4E3D856sBr5TR46I1AH8KoD/NJ5yNoDNAF6J7s3hJgAXlj9jAP5pAG6SHsBgj5e/E5Ec3YA9CuCXjOcVAG5S1SYAiMh7APyWqk4DmBaRPwfw0UE4TNYHP8bHyzWquhFADcANAP5dRM4OPO9o+VH/FOcAeHbJ38/0z0XSSxjskaOqHVW9E92V+ctDT3nR34cAnLfk7/P75RvpLQz2yJEuVwPYBODRFQy5A8AfiMgmETkXwG/31UHSMxjs8fL3IjIHYAbAnwC4TlUfXsG4j6H70f1HAL4N4Mv9c5H0EmHxCkLigHd2QiKBwU5IJDDYCYkEBjshkTDQb9BVR8e0vmHT6geKBDeveXFxzWuSg1vMLJzXpkVhDzTmKry13N8Kffr/h7L3KhK+jySJN8Y+lnuqvbkyTOq9am9/jhveHLsvzthr4Zxny9aan0XeXAwebF3BLiJXAvgsgBTAX6nqJ7zn1zdswlt//XeCNi9wkyQNbs873km2bd4kLnNVObbV78+74JrNpmlrNBqmLU3Dc5UacwgARbG2cK9kmWmrjtaC20dGRpz92R80i459zjrtjmnL8zy8P+ca6HTCY7z9AUClYodTkjgfojXsy8LCgjnEsj1279dsH2wPfEQkBfCXAN6FbsbUtSKyc637I4T0l/X8z34pgCdV9WlVbQG4HcDVvXGLENJr1hPs23F6QsSBcttpiMguEZkSkanW4vw6DkcIWQ99X41X1d2qOqmqk9XRsX4fjhBisJ5gP4jTs5/OLbcRQl6CrGc1/vsALhKRHegG+XsB/Jo3QFXRarWCtrzdNscladjNJFnb6mfiyCCe5GXKHcZrAoC287o86aper9t+OCvTi8ZKfaVqr8aPjtrH6rir1vYqeMtQEzzZMOs459MTttSR84xx7aZ9XuYXZ01bntvj6qOjpq1atVWIRmMxuH12ds4ZEz7P3rWx5mBX1VxEbgDwL+hKb7euMGuKEDIE1qWzq+rdAO7ukS+EkD7Cr8sSEgkMdkIigcFOSCQw2AmJhIFmvXU6HZw4ccK0WViyxfj4hDkmEVtq8hJQUi9hwRhnJZ8si5N/klXsJJORM2wZZ2zckF6cRBhPAmw7UpN6Mk8lPI+pk/XmyXK5l6zj5jWFx3nZd5XUvgZE7Xn09unZsqwa3D7qSHlW0k3i+M47OyGRwGAnJBIY7IREAoOdkEhgsBMSCQNdjS+cRBhvRbhpJArAWaGt1cJlkYBlkmSclXXL5pZacksVOSvkTnJHYZQxAgAxVsgbTi0BK2kFWHudv9RIXqpk9nykqfOanVJRi4t2+SarjJSVINM12q/ZK1k1c9K4TgG3Bl2ShJWXNZVWc04X7+yERAKDnZBIYLATEgkMdkIigcFOSCQw2AmJhIFKb2ma4oyN4fZP4rbjCW8uvBpoLVtOynN7nJf4kTuJHxapk5hQrdry4FjdrsSbVe0kmY4hNbUs+RLA3Kxdc82TfzKnrlp1JPzaPNmwKOzz0li067E1m/ZrsxQvT34Vz+ZIdupJos591UoC8+RoM/nKk/hMCyHkZQWDnZBIYLATEgkMdkIigcFOSCQw2AmJhIFKb0mSYLQWbjXkNbm31ARPmvDqkrnlzArHjyQ8UGT1desAAI4sVzOkKwAYMWqWATBf3NiY3eKp5tQ689pXZVXbD0va8moNetdAmm4wbRMbbJtq+HjNVrh9EgA0G3Y7LyubD/AzLT0axvG8ufeyMy3WFewish/ALIAOgFxVJ9ezP0JI/+jFnf0XVPVYD/ZDCOkj/J+dkEhYb7ArgG+LyIMisiv0BBHZJSJTIjLVcqqlEEL6y3o/xl+uqgdF5BUA7hGR/1bV+5Y+QVV3A9gNAGdsPXdtNY4IIetmXXd2VT1Y/j4C4JsALu2FU4SQ3rPmO7uIjAFIVHW2fPxOAH/sDlI1pRevaKCF1yKp4ti897jUHRf23cugyjJ7f9mII105cl7esiUZK+utcLLXUqcoppdF5RXTtGRRT3qzipECfvad11opz8P79I7VcbIbLSmvu0/7vHhYxSM9abljZW4687Sej/FbAXyzdKgC4G9U9Z/XsT9CSB9Zc7Cr6tMA3tBDXwghfYTSGyGRwGAnJBIY7IREAoOdkEgYaNabwi4q6GawGZljhdpZUp3CkYxSW/JSq4cWgNzo8+XJZOr0bPOy5bzChl5RTFs2siWZqpO9ZvYUgy+HWf3v6nU7+87zw5PKMqd/nBh927yCpM2mk23m9OcbHfX6C9rjGkYxUC8L0JJS3V6FpoUQ8rKCwU5IJDDYCYkEBjshkcBgJyQSBrsar4rcWEmuVOxVRK8tkEXHWDkHgE6++jZOgJ0TkhsJMgDQbNqryLMnZ0ybl7iizsq61booc+bXq53mre567beslWRPdfFW972km6zi1OQzVuN9vNV9e1SSrG2c9dq8uTLn1z4M7+yExAKDnZBIYLATEgkMdkIigcFOSCQw2AmJhIFKb5U0xebNG4M2t96WlYDiJBd4IkTuSG+5VdsLQMtIkHBrp3m9phy8+nritI0qDKlJHFnL8z91pDfPZkuA9utKndelXjsvL1nHmH8R2/cs8+RB2+bVIvSu77VIb9a1744xLYSQlxUMdkIigcFOSCQw2AmJBAY7IZHAYCckEgYqvSWJoFYL1yZTR1tRDcsMngxSOJJXmtove3TUyfKqh330aoV5traTESdOSyPPpobk6GWvaWH76LU08ua4YrjotU9SJ7tRxD5nnuRlZQ/mbU9i9c6LfaxEwtc24Gcqto16eI3FcG06AGgacqnXXmvZO7uI3CoiR0Rk35Jtm0XkHhF5ovy9abn9EEKGy0o+xn8RwJUv2nYjgHtV9SIA95Z/E0Jewiwb7GW/9ekXbb4awG3l49sAXNNbtwghvWatC3RbVfVQ+fh5dDu6BhGRXSIyJSJTjYW5NR6OELJe1r0ar91aQubqg6ruVtVJVZ2s1cfXezhCyBpZa7AfFpFtAFD+PtI7lwgh/WCt0ttdAK4D8Iny97dWMqjb/iksX6nTyslK2Gq3bTnDlSAc+cRJDnOzq8wx3g6dDKVW05ZdWm27dVGrZWR5ORllFadgZuHIchAnM8/IKitSRxJ1qiVWM1vWSp1MurXgdOVCnjuynFPcsuLIvVXDf606BT2da8BiJdLbVwH8B4DXiMgBEbke3SB/h4g8AeAXy78JIS9hlr2zq+q1huntPfaFENJH+HVZQiKBwU5IJDDYCYkEBjshkTDYXm+Fot0ISxdZxX7fqVYtacUrHOn0enNkOXF0Fy3C2pAnyXl+FB17XMspfLlgzCEA5O3wPqVtS3n5/HHTps64rDZh2tKJLeH9jdh95SpVx+ZkvVUcCdMqiunJr/b15vcd9HrVpU5x1IrRI87rwbfJKNz6uOM77+yERAKDnZBIYLATEgkMdkIigcFOSCQw2AmJhIFKb+12G4eeOxS0iSOjZSPV4PYRJz8+yzz5xD5W4chy1nuj1/PMez9td5zMJUdOqtfHTFtmSEqNYzPmmOkTz5u2c7acYdpe91OvMm0P7z8c3F5URs0xMHqeAf55aTtFLD3p03ZjbffAdtsuzunJvakhK1ar9nxkWdjmJVnyzk5IJDDYCYkEBjshkcBgJyQSGOyERMKA2z8lqI+HkyeaTs213EgYqTgrrf4KuU3htKHqGMkpXgKE1e6qa3NW6o32PgCQO0kyeWs+vH3+BXPMlgn7Mnj1+WaVcGw/01YF7r//yeD2RrbZHLPx7PNNW9VRIOYX7blaS91Aa6UbAFKnlp+bfOWcM6u1Vatt+1GrhRUq7/Xyzk5IJDDYCYkEBjshkcBgJyQSGOyERAKDnZBIGKj0BhEURqJGNmrX27KkkJpXs8xJqhAnycSTT+bmFoLbW+2GOQZiSy6Z2MeqOfLPyJhd++3kcUN6S+1j7dh+jmkb31A3bdMn7dp1Z2zaFNy+cNIcgqJjS5jValhqAoDKiC1vWufak6jUkV+TxL52xsZseRBqj7NcyTtOEo/lh3Ntr6T9060ickRE9i3ZdrOIHBSRPeXPVcvthxAyXFbyMf6LAK4MbP+Mql5c/tzdW7cIIb1m2WBX1fsATA/AF0JIH1nPAt0NIrK3/Jgf/gcNgIjsEpEpEZlqLYb/nySE9J+1BvvnAFwI4GIAhwB8ynqiqu5W1UlVnayOOgsYhJC+sqZgV9XDqtrR7rLlFwBc2lu3CCG9Zk3Sm4hsU9VTxeTeDWCf9/xTpEmCjYaUk7ftzCWrflfHqfmlTpaRJ8t5LZms1lBtx/f5eftfl6Jp14XLF+xxGzea/zWh6ITlwYX5WXNMkp5p2l6387WmLR2169PNVcLtn7a3nKwxsTMVG84ce4ltlUp4n15WZKPptdey5bDMua5GnLZXVtakd+20WoYfTgbmssEuIl8FcAWALSJyAMBNAK4QkYsBKID9AN6/3H4IIcNl2WBX1WsDm2/pgy+EkD7Cr8sSEgkMdkIigcFOSCQw2AmJhIFmvaWJYMLKbnOy3iw1oSjsDB+vFU9iSGgAkGW2jlPJwnJNfcSexuMv2Blxzzz7rGk7efg501Z1MuKyWtjHpGnLOI8n9v7O3Py0aRvdeJZpmzUkNkm9rCxbLq05c+xllFlZb4mRfdn1w86i06pt8/bpIuFrbrxux0THSAJME1tS5J2dkEhgsBMSCQx2QiKBwU5IJDDYCYkEBjshkTBQ6S3vFDh2PJx9lTiF8hRh7c3rsSZYvRwDAHlhS2VFOyxfdebmzDEv7P8f0zbznG0rGrYfi05BxAVDeRmp2AUbf/TMEdN29Oi/mbbX/8ykacOmc4ObxckMywxpEwCKwj7XZgYYgDwPS7Ctli3NWtmNAFCpZKbNK2LpZUbmediWpvZcVY1iq0Vhy5e8sxMSCQx2QiKBwU5IJDDYCYkEBjshkTDQ1XgFYC2qipNE0G6FVyvbxkor4Lfw8Vo8NZvhGm4AMHMknLiSzNptkM4/6xWm7ZWXXGLaHnv8CdPWMFZvAeCc7duD2yfGxs0xozWvjZatXExs2GjajrcXg9uPHrPnV71ics5qfO7UDfRWyC0SL5nEuU6968rzwxKHvGNVsvA85ka9RoB3dkKigcFOSCQw2AmJBAY7IZHAYCckEhjshETCSjrCnAfgSwC2oque7VbVz4rIZgBfA3ABul1h3qOqtgaFrrTSajSDto7zBf4Fow1OqxXe13J4xyoc6WJDPSxf/fylP2mOefMlrzdtOy54jWnb+8hjpu3I9Aum7bLL3hLcrm17rl5w9je+wZbsjs/aLaX+8Tv3B7cfnrblKRU7ySTB6iW07k7Dkp0nUcGphee1jVprDTorkafTcZJ1krBeZ7VKA1Z2Z88BfFhVdwK4DMAHRWQngBsB3KuqFwG4t/ybEPISZdlgV9VDqvqD8vEsgEcBbAdwNYDbyqfdBuCaPvlICOkBq/rcISIXALgEwAMAti7p5Po8uh/zCSEvUVYc7CIyDuAbAD6kqqf1GtZuFYngP0cisktEpkRkqt2wvypJCOkvKwp2EcnQDfSvqOqd5ebDIrKttG8DECx3oqq7VXVSVSezWrg3OyGk/ywb7NKt4XQLgEdV9dNLTHcBuK58fB2Ab/XePUJIr1hJ1tvPAXgfgIdEZE+57SMAPgHgDhG5HsAzAN6z3I46nQ5mT4bVOSuzDQAKI4PNq0vm1afz6Di1wlAJ14V73WvPMYecv2PCtM0ce9K0veWSC01bMrLTtL3CyLJrGVloAHB02pa8Zubt+npj9THTtuPsjcHtDz1xwByTjobHAH7tt1Rtucm6DBLn2km8FlWOvLbWa65i1Jrz2nx1irAs55RXXD7YVfW7gFm98e3LjSeEvDTgN+gIiQQGOyGRwGAnJBIY7IREAoOdkEgYaMHJNE2xYcOGsM3I4gGAxGqD4+gMCwvOt/WccYkhaQBAMfN8cPtzBw6bY8Zqtiy05/49pm3HBT9h2t78s28ybflY+P378BHbx7kFW147ORvOOASAjtr3iiwJy1BtJ1OxndgZcVa7IwCQilOMsh0+n5Z0BQCp2GFRyZz2T470pk42mhoZfZ21FNl01D/e2QmJBAY7IZHAYCckEhjshEQCg52QSGCwExIJA5XeRMTMXnKzdSphN1NHBslGqvYOHXmiktrvf8lEuPjis06ZzYWnZmzbyNmmbd9BWw5bfNDuA3fRdPh4Cwt2ccjZmROm7YXjJ01bkdhz/Px0+HgTGzebY0Y32MWOajXvfNqSXWpM/9yMfV6qVftY9TE70y9zrkevEKRl88a0jCzRxCuIaVoIIS8rGOyERAKDnZBIYLATEgkMdkIiYaCr8Z1OBydOngjaEmc5PjVW472VR68emLWSCbgL9ZgYD6/EFpVRc8zxY/b+OvmIactze4V5+ik7qeXZY+HV88XZYPFfAEAqdmul+ng4cQkAWk6ixmwePjebtoRr5AFAUnGSXZz2T3Pzdn29hflwQtTiYrieIACkqb2q7spGDtY1DABpFl7991pUVUbCc8XVeEIIg52QWGCwExIJDHZCIoHBTkgkMNgJiYRlpTcROQ/Al9BtyawAdqvqZ0XkZgC/AeBo+dSPqOrd3r6KosDiYlgmqTiSAZrhumWeNOFJV22jLhkAFB3btjgXljvam205qT5+hmkza+sBQMV+H27aKhSmZ8LzWyzacuNoZh/r6DE7y2cht8c1JNzEM02dhBZHAtTCtqVOu6aRWviciXOfsxKvACB12j95raHca9V4bd7+ZA0S4Ep09hzAh1X1ByIyAeBBEbmntH1GVf9s1UclhAyclfR6OwTgUPl4VkQeBbC9344RQnrLqv5nF5ELAFwC4IFy0w0isldEbhWRTb12jhDSO1Yc7CIyDuAbAD6kqjMAPgfgQgAXo3vn/5QxbpeITInIVN60v9ZICOkvKwp2EcnQDfSvqOqdAKCqh1W1o6oFgC8AuDQ0VlV3q+qkqk5WRuzvkBNC+suywS7dZb9bADyqqp9esn3bkqe9G8C+3rtHCOkVK1mN/zkA7wPwkIjsKbd9BMC1InIxunLcfgDvX25HXvsnOFlqFh1HXlvD7gD42VWW3NFxDtZ2Muwmxu2sNxX7fbjTsV/3otHWKEvsjLJGbstCi237WEXF8d+YxjS156pStS9HcdqDVTLbVs3CPhZt+zV7mW2VzAkZ55rzpGCrNZSjNpqtobxsz5Wsxn8XQOjVu5o6IeSlBb9BR0gkMNgJiQQGOyGRwGAnJBIY7IREwkALTnYJyxp5bmebWW1w2k0ne61wpBWHxCk2mFbDMk41cabRkXHanvzjtKHKc1uTOTkXbhvVnLPbP8GZq6RiZ6l1NJyNCACddtiW1ewvVm3YvMW0VUZsma/RsItHdvKwFFUR+5x1nPnwMia9dk2Nhv3t0YaRCSqO/DpizYcjvfHOTkgkMNgJiQQGOyGRwGAnJBIY7IREAoOdkEgYqPSmameIiVN8sWb0wkodyavVtLPNPPnES5arGEUgnaQrJGLv0cpcAoDG4rxpmzH65QFAezEsvaEV7nkGAOpITVVH8vL6wGW1ibAbzmuefsGWB+sTto+qtq3ohI+XVO37nDpXQeHIa4WTplY1ZFsASKzr2LkYrfPiFanknZ2QSGCwExIJDHZCIoHBTkgkMNgJiQQGOyGRMFDpTQSw6vWpkwGWGX3g1Cn+Vzi6RZLZfeU86cKSqDqOlJc7+0NhFyFMcls6HHcKMxbJWNgwZhecdCUjp++ZJyflRsbW+KjhH4BsxPax5RRstOQ1AMiycBajVwAy79hz72Wite1hKBzJ0eot50nECwthadY7l7yzExIJDHZCIoHBTkgkMNgJiQQGOyGRsOxqvIjUANwHYKR8/tdV9SYR2QHgdgBnAngQwPtU1VmPBKqVFOecFe7snDirlUURzjTxVtUXFuzEDxenZtxiI/zyqsaKLwDU63V7f/NG0goASe3ab9UJe9XamsXDs3ZijVdDrzBeMwBMn7T32TBWtGs1+7xs3Gh3/RanNqC3Qm61Qyqs/lRdq2Ozj5U6yVwdp8ZimoV9rNdttaMwXtd6E2GaAN6mqm9Atz3zlSJyGYBPAviMqr4awHEA169gX4SQIbFssGuXU7egrPxRAG8D8PVy+20ArumHg4SQ3rDS/uxp2cH1CIB7ADwF4ISqnvpmwgEA2/viISGkJ6wo2FW1o6oXAzgXwKUAXrvSA4jILhGZEpEpryADIaS/rGo1XlVPAPgOgLcA2Cjyf5X2zwVw0BizW1UnVXWy5nxVkhDSX5YNdhE5S0Q2lo9HAbwDwKPoBv2vlE+7DsC3+uQjIaQHrCQRZhuA20QkRffN4Q5V/QcReQTA7SLycQD/BeCW5XaUpAk2GAkZlcSW0U7Ohdvj1Ku2TFav2G2Gmk27bVHFqHcHAGduCH8y6ThJFXBq0DWd2nVw5DBxWkMlhgwlqZNI4rznLzRtqcxR5TAyfmZwuycNzc7aO0xSr26gLZVZiTAjI/Z5dnJJ0GzabZwqjjxodD3rjjOuuZpT/8+SiL35XTbYVXUvgEsC259G9/93QsiPAfwGHSGRwGAnJBIY7IREAoOdkEhgsBMSCWJlBfXlYCJHATxT/rkFwLGBHdyGfpwO/TidHzc/XqmqZ4UMAw320w4sMqWqk0M5OP2gHxH6wY/xhEQCg52QSBhmsO8e4rGXQj9Oh36czsvGj6H9z04IGSz8GE9IJDDYCYmEoQS7iFwpIo+JyJMicuMwfCj92C8iD4nIHhGZGuBxbxWRIyKyb8m2zSJyj4g8Uf62S63214+bReRgOSd7ROSqAfhxnoh8R0QeEZGHReR3y+0DnRPHj4HOiYjUROR7IvLD0o+Pldt3iMgDZdx8TUTsPN0QqjrQHwApujXsXgWgCuCHAHYO2o/Sl/0AtgzhuG8F8EYA+5Zs+1MAN5aPbwTwySH5cTOA3xvwfGwD8Mby8QSAxwHsHPScOH4MdE7QzX4fLx9nAB4AcBmAOwC8t9z+eQC/uZr9DuPOfimAJ1X1ae3Wmb8dwNVD8GNoqOp9AKZftPlqdKv0AgOq1mv4MXBU9ZCq/qB8PItuJaTtGPCcOH4MFO3S84rOwwj27QCeXfL3MCvTKoBvi8iDIrJrSD6cYquqHiofPw9g6xB9uUFE9pYf8/v+78RSROQCdIulPIAhzsmL/AAGPCf9qOgc+wLd5ar6RgDvAvBBEXnrsB0Cuu/ssJu79JvPAbgQ3YYghwB8alAHFpFxAN8A8CFVnVlqG+ScBPwY+JzoOio6Wwwj2A8COG/J32Zl2n6jqgfL30cAfBPDLbN1WES2AUD5+8gwnFDVw+WFVgD4AgY0JyKSoRtgX1HVO8vNA5+TkB/DmpPy2CewyorOFsMI9u8DuKhcWawCeC+AuwbthIiMicjEqccA3glgnz+qr9yFbpVeYIjVek8FV8m7MYA5ERFBt2Dpo6r66SWmgc6J5ceg56RvFZ0HtcL4otXGq9Bd6XwKwB8OyYdXoasE/BDAw4P0A8BX0f042Eb3f6/r0W2QeS+AJwD8K4DNQ/LjywAeArAX3WDbNgA/Lkf3I/peAHvKn6sGPSeOHwOdEwA/jW7F5r3ovrF8dMk1+z0ATwL4WwAjq9kvvy5LSCTEvkBHSDQw2AmJBAY7IZHAYCckEhjshEQCg52QSGCwExIJ/wvAkXSzSjHcJQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the entire data set\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Loop through each picture in the data set\n",
    "for i in range(1000):\n",
    "    # Grab an image from the data set\n",
    "    sample_image = x_train[i]\n",
    "    # Grab the image's expected class id\n",
    "    image_class_number = y_train[i][0]\n",
    "    # Look up the class name from the class id\n",
    "    image_class_name = cifar10_class_names[image_class_number]\n",
    "    \n",
    "    if i==47:\n",
    "        # Draw the image as a plot\n",
    "        plt.imshow(sample_image)\n",
    "        # Label the image\n",
    "        plt.title(image_class_name)\n",
    "        # Show the plot on the screen\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa56d809-ed1c-4641-b148-11aa315ebf3d",
   "metadata": {},
   "source": [
    "### Load Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74406864-0e3f-4b93-a687-23c2af012eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e5fe43-b25b-4632-9b65-84cc4b300bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "(x_train, y_train), (x_test,y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb126f4f-be96-49f8-8d1b-51c4e8e0da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data set to 0-to-1 range\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_train.astype(\"float32\")\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f19ee123-5e51-4a78-a02a-bd7b838371bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "# Our labels are single values from 0 to 9.\n",
    "# Instead, we want each label to be an array with on element set to 1 and and the rest set to 0.\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b37ab-aa20-4a48-83e1-6e8532559a30",
   "metadata": {},
   "source": [
    "### Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba567c47-e048-4edd-a6a2-ec6cea6eb906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6182f0b-b08e-46e6-a2b5-aa455b3536b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d394e81-e35d-4122-936b-3ed0306f5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data set to 0-to-1 range\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "349da732-346b-43cd-abfd-20275129422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c85a9ba1-6c13-4f3b-a05f-4a52ab774237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(523, activation=\"relu\", input_shape=(32,32,3)))\n",
    "model.add(Dense(10,  activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a856c92-6a37-4f41-8cb6-9738e1e74d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32, 32, 523)       2092      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32, 32, 10)        5240      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,332\n",
      "Trainable params: 7,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d661d3e3-afd3-4047-9d37-b8e4b7c7d765",
   "metadata": {},
   "source": [
    "### Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3655b3c-7ff3-4e83-ad86-93b3aeb46478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9e6b35a-219c-42e9-b79c-0be044ef00dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10f3aefb-c903-4711-8ee7-b7f53a86e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data set to 0-to-1 range\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aea4d470-9f7d-4031-8889-b08f7803f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "077d64cc-289e-4db9-a1e9-5a4cbfe5d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model and add layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", activation = \"relu\", input_shape=(32, 32, 3)))\n",
    "model.add(Conv2D(32, (3,3), activation = \"relu\"))\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\", activation = \"relu\"))\n",
    "model.add(Conv2D(64, (3,3), activation = \"relu\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation=\"relu\", input_shape=(32, 32, 3)))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69cc671d-320c-4bc7-9e72-ca0913801634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 30, 30, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50176)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               25690624  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,761,322\n",
      "Trainable params: 25,761,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000a5bb-b9a5-4de9-a34a-2a9079317bd5",
   "metadata": {},
   "source": [
    "### Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13005293-3e7e-4f13-99cb-1bab0e9ce540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f170cee-9a11-4275-becf-021a27b50165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f61cef63-276d-4b21-8d86-36283e82607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data set to 0-to-1 range\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90025d1c-401d-4496-a68b-f071c6a06f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6785150a-7f04-43f7-ab20-3ec8e9678831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model and add layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3), activation=\"relu\"))\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db2ce503-9bdf-4169-b9cd-7e1132136cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               1180160   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f348fc4-79e1-4528-bd96-8136d39863f7",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85eaaa65-516c-42ba-a8c1-c255955d9eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfee965d-cf47-46cf-9685-6379ccb1f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ac5fc4d-e853-4c64-8dd2-6314c5c9db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data set to 0-to-1 range\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97e584f4-82c1-4874-aedb-d8258ec30245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18dac36c-2d4b-41ce-9c3d-d1f9447948ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model and add layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3), activation=\"relu\"))\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da0c7029-84e8-4f11-b019-bf7a6d2de62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 15, 15, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 15, 15, 32)        0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 15, 15, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 13, 13, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               1180160   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb163de1-a16c-43d9-81e5-a68bd93d637c",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d69b5468-1008-461c-98f7-93962312f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4923985a-b066-4bcd-85e9-138e721e3739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 15, 15, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 15, 15, 32)        0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 15, 15, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 13, 13, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               1180160   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15447881-2256-41af-a947-d275f79dcf89",
   "metadata": {},
   "source": [
    "## Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ea9fe56-f075-4b1e-a48c-389178694e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 1.4974 - accuracy: 0.4552 - val_loss: 1.1307 - val_accuracy: 0.5986\n",
      "Epoch 2/30\n",
      "1563/1563 [==============================] - 66s 42ms/step - loss: 1.0985 - accuracy: 0.6098 - val_loss: 0.9371 - val_accuracy: 0.6749\n",
      "Epoch 3/30\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.9507 - accuracy: 0.6650 - val_loss: 0.8515 - val_accuracy: 0.6996\n",
      "Epoch 4/30\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.8501 - accuracy: 0.7005 - val_loss: 0.7800 - val_accuracy: 0.7265\n",
      "Epoch 5/30\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7922 - accuracy: 0.7235 - val_loss: 0.7679 - val_accuracy: 0.7335\n",
      "Epoch 6/30\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.7441 - accuracy: 0.7404 - val_loss: 0.7053 - val_accuracy: 0.7548\n",
      "Epoch 7/30\n",
      "1563/1563 [==============================] - 66s 42ms/step - loss: 0.7015 - accuracy: 0.7559 - val_loss: 0.6807 - val_accuracy: 0.7671\n",
      "Epoch 8/30\n",
      "1563/1563 [==============================] - 65s 42ms/step - loss: 0.6662 - accuracy: 0.7654 - val_loss: 0.6850 - val_accuracy: 0.7674\n",
      "Epoch 9/30\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 0.6427 - accuracy: 0.7752 - val_loss: 0.6750 - val_accuracy: 0.7725\n",
      "Epoch 10/30\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 0.6210 - accuracy: 0.7812 - val_loss: 0.6611 - val_accuracy: 0.7768\n",
      "Epoch 11/30\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 0.5892 - accuracy: 0.7918 - val_loss: 0.6764 - val_accuracy: 0.7733\n",
      "Epoch 12/30\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 0.5763 - accuracy: 0.7969 - val_loss: 0.6674 - val_accuracy: 0.7783\n",
      "Epoch 13/30\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 0.5617 - accuracy: 0.8024 - val_loss: 0.6532 - val_accuracy: 0.7813\n",
      "Epoch 14/30\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 0.5466 - accuracy: 0.8094 - val_loss: 0.6680 - val_accuracy: 0.7767\n",
      "Epoch 15/30\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 0.5307 - accuracy: 0.8134 - val_loss: 0.6918 - val_accuracy: 0.7683\n",
      "Epoch 16/30\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.5189 - accuracy: 0.8178 - val_loss: 0.6472 - val_accuracy: 0.7858\n",
      "Epoch 17/30\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.5013 - accuracy: 0.8245 - val_loss: 0.6482 - val_accuracy: 0.7881\n",
      "Epoch 18/30\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.4994 - accuracy: 0.8247 - val_loss: 0.6547 - val_accuracy: 0.7877\n",
      "Epoch 19/30\n",
      "1563/1563 [==============================] - 65s 42ms/step - loss: 0.4853 - accuracy: 0.8284 - val_loss: 0.6537 - val_accuracy: 0.7825\n",
      "Epoch 20/30\n",
      "1563/1563 [==============================] - 65s 41ms/step - loss: 0.4732 - accuracy: 0.8361 - val_loss: 0.6513 - val_accuracy: 0.7839\n",
      "Epoch 21/30\n",
      "1563/1563 [==============================] - 66s 42ms/step - loss: 0.4764 - accuracy: 0.8325 - val_loss: 0.6518 - val_accuracy: 0.7900\n",
      "Epoch 22/30\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 0.4542 - accuracy: 0.8403 - val_loss: 0.6429 - val_accuracy: 0.7871\n",
      "Epoch 23/30\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.4512 - accuracy: 0.8408 - val_loss: 0.6744 - val_accuracy: 0.7839\n",
      "Epoch 24/30\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.4446 - accuracy: 0.8462 - val_loss: 0.6822 - val_accuracy: 0.7840\n",
      "Epoch 25/30\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.4407 - accuracy: 0.8461 - val_loss: 0.6969 - val_accuracy: 0.7755\n",
      "Epoch 26/30\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.4359 - accuracy: 0.8469 - val_loss: 0.6615 - val_accuracy: 0.7885\n",
      "Epoch 27/30\n",
      "1563/1563 [==============================] - 66s 42ms/step - loss: 0.4293 - accuracy: 0.8500 - val_loss: 0.7306 - val_accuracy: 0.7746\n",
      "Epoch 28/30\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.4284 - accuracy: 0.8511 - val_loss: 0.7085 - val_accuracy: 0.7762\n",
      "Epoch 29/30\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.4180 - accuracy: 0.8529 - val_loss: 0.6951 - val_accuracy: 0.7844\n",
      "Epoch 30/30\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 0.4130 - accuracy: 0.8566 - val_loss: 0.6476 - val_accuracy: 0.7911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x156b4667df0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(x_test, y_test),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af7a64b9-56b2-4b34-8d2d-906038aae4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4397"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Savve Neural Network Structure\n",
    "model_structure = model.to_json()\n",
    "f = Path(\"model_structure.json\")\n",
    "f.write_text(model_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b684f90-6bf7-410c-9cf8-7714d287711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Neural Network trained weights\n",
    "model.save_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b82d82-aa7b-4089-8d42-f0b167e0f5e1",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "169a5e67-7410-4128-8917-409d4e2e060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ff98576-25ee-466f-9cb9-b91fd715ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the CIFAR10 class labels from the training data (in order from 0 to 9)\n",
    "class_labels = [\n",
    "    \"Plane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Boat\",\n",
    "    \"Truck\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28567e64-f78a-4795-a33c-9749c66e51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json file that contains the model's structure\n",
    "f = Path(\"model_structure.json\")\n",
    "model_structure = f.read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b10927df-a4ba-4ed5-a3ac-e0259568a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the Keras model object from the json data\n",
    "model = model_from_json(model_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7aceb9cd-6e50-4477-bc06-2913e5cd9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model's trained weights\n",
    "model.load_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a376fa6f-7aab-4460-90b8-5f39a2d3ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image file to test, resizing it to 32x32 pixels (as required by this model)\n",
    "img = image.load_img(\"./04/cat.png\", target_size=(32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b575074-e0da-4748-b3f4-4834bcbf528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to a numpy array\n",
    "image_to_test = image.img_to_array(img) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c514a722-6922-437e-ae22-178925af4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a fourth dimension to the image (since Keras expects a list of images, not a single image)\n",
    "list_of_images = np.expand_dims(image_to_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "970b2393-5027-40aa-b319-5fc9f4ded6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction using the model\n",
    "results = model.predict(list_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "673b661c-9d6c-406f-b2ba-0db165225c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are only testing one image, we only need to check the first result\n",
    "single_result = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f0fdcaf-24de-4a38-bbee-86fe1b359e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will get a likelihood score for all 10 possible classes. Find out which class had the highest score.\n",
    "most_likely_class_index = int(np.argmax(single_result))\n",
    "class_likelihood = single_result[most_likely_class_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf4d94d0-563a-4ad3-bc87-b5d27ae68cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of the most likely class\n",
    "class_label = class_labels[most_likely_class_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e143d553-717d-46f1-9f01-b421336f5358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is image is a Cat - Likelihood: 0.943206\n"
     ]
    }
   ],
   "source": [
    "# Print the result\n",
    "print(\"This is image is a {} - Likelihood: {:2f}\".format(class_label, class_likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29637351-29d0-4280-a46a-afaaa5f3ecd0",
   "metadata": {},
   "source": [
    "## Fine-tuning Pre-trained Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1c1e5-26aa-452d-b786-739317a4260b",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77bd30ea-96e3-4702-9837-060973f6ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61329e14-4698-4134-a1cd-dd4241ed58c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 7s 0us/step\n",
      "553476096/553467096 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load Keras' VGG16 model that was pre-trained against the ImageNet database\n",
    "model = vgg16.VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2396620-5541-42af-9727-25aca7410fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image file, resizing it to 224x224 pixels (required by this model)\n",
    "img = image.load_img(\"./05/bay.jpg\", target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8128cf3e-e896-4854-bf31-7f9f1da1bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to a numpy array\n",
    "x = image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92ab5573-34c3-4e59-bc7e-92e0c7feb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a fourth dimension (since Keras expects a list of images)\n",
    "x = np.expand_dims(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "89592c10-f8c5-4d7e-a695-7f6fe81886b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input image's pixel values to the range used when training the neural network\n",
    "x = vgg16.preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8424e018-1efa-4ec8-aa4b-ca9b379e4f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the image through the deep neural network to make a prediction\n",
    "predictions = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cfdbb750-1059-4114-8172-908a77d115fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up the names of the predicted classes. Index zero is the results for the first image.\n",
    "predicted_classes = vgg16.decode_predictions(predictions, top=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d423aeba-9ef4-4784-9a61-a3c4ea2808df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predictions for this image:\n",
      "Prediction: seashore - 0.395213\n",
      "Prediction: promontory - 0.326129\n",
      "Prediction: lakeside - 0.119613\n",
      "Prediction: breakwater - 0.062801\n",
      "Prediction: sandbar - 0.045267\n",
      "Prediction: cliff - 0.011845\n",
      "Prediction: dock - 0.009196\n",
      "Prediction: boathouse - 0.003278\n",
      "Prediction: valley - 0.003194\n"
     ]
    }
   ],
   "source": [
    "print(\"Top predictions for this image:\")\n",
    "\n",
    "for imagenet_id, name, likelihood in predicted_classes[0]:\n",
    "    print(\"Prediction: {} - {:2f}\".format(name, likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404b70b7-c386-41ea-95dd-b28e7dee2c4d",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59a3707c-230f-47fe-8f8b-cd288ac40777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import joblib\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ff27a45-db2d-400f-b655-1f862065176c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05\\training_data\\not_dogs\n"
     ]
    }
   ],
   "source": [
    "# Path to folders with training data\n",
    "dog_path = Path(\"./05/training_data\") / \"dogs\"\n",
    "not_dog_path = Path(\"./05/training_data\") / \"not_dogs\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "print(not_dog_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1086b531-0142-4cc6-a033-7255b221d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the not-dog images\n",
    "for img in not_dog_path.glob(\"*.png\"):\n",
    "    # Load the image from disk\n",
    "    img = image.load_img(img)\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    image_array = image.img_to_array(img)\n",
    "\n",
    "    # Add the image to the list of images\n",
    "    images.append(image_array)\n",
    "\n",
    "    # For each 'not dog' image, the expected value should be 0\n",
    "    labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc83f77d-0f0f-4f59-9279-dfdcf4baa94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the dog images\n",
    "for img in dog_path.glob(\"*.png\"):\n",
    "    # Load the image from disk\n",
    "    img = image.load_img(img)\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    image_array = image.img_to_array(img)\n",
    "\n",
    "    # Add the image to the list of images\n",
    "    images.append(image_array)\n",
    "\n",
    "    # For each 'dog' image, the expected value should be 1\n",
    "    labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8ec325a-9d54-434c-916f-b6d2493db39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single numpy array with all the images we loaded\n",
    "x_train = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05aa81f8-a6a0-4c1f-b8ab-ab87bb6009e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also convert the labels to a numpy array\n",
    "y_train = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7744a3c3-93f8-4916-8b56-2630dddd0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize image data to 0-to-1 range\n",
    "x_train = vgg16.preprocess_input(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c9025841-4933-4776-8a1f-a93b28e30bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "58900480/58889256 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained neural network to use as a feature extractor\n",
    "pretrained_nn = vgg16.VGG16(weights=\"imagenet\", include_top=False, input_shape=(64, 64, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7af41ba-b319-4ce0-b177-3eb8d088b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for each image (all in one pass)\n",
    "features_x = pretrained_nn.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6b1a9ed-ab25-4116-be6e-c499fd67d347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_train.dat']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the array of extracted features to a file\n",
    "joblib.dump(features_x, \"x_train.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "96a1464b-7028-4f2b-999b-9eb4c66d05ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_train.dat']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the matching array of expected values to a file\n",
    "joblib.dump(y_train, \"y_train.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c481826-e69a-4f74-a82b-47edccaf0ba1",
   "metadata": {},
   "source": [
    "### Training with extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e1d4146f-4398-4d78-a272-275c699fe088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from pathlib import Path\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7bab04e5-6adf-441f-9f6d-5aee54ae9a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "x_train = joblib.load(\"x_train.dat\")\n",
    "y_train = joblib.load(\"y_train.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6a695f56-0d44-48f4-8f06-6dbb860df374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model and add layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "253b852a-beca-42d8-97f3-d15d524bcb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8af60a8-53b8-4de9-9a8f-9c72c3522842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 12.0044 - accuracy: 0.6379\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.9970 - accuracy: 0.8448\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.3389 - accuracy: 0.9655\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6939 - accuracy: 0.9655\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6346 - accuracy: 0.9655\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5743 - accuracy: 0.9655\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 0.9828\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7288 - accuracy: 0.9828\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0501 - accuracy: 0.9828\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.2920e-12 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x156b95808b0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d5dab63c-c4d0-4699-b94b-cce983ab5406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1459"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save neural network structure\n",
    "model_structure = model.to_json()\n",
    "f = Path(\"model_structure.json\")\n",
    "f.write_text(model_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b843602-907b-4cd1-89ad-efbe0b5ae83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save neural network's trained weights\n",
    "model.save_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee8d03b-11e8-4598-8130-b55fd1fb9df3",
   "metadata": {},
   "source": [
    "### Making Predictions with Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0b6e2ec5-d09e-4339-8a03-f9b1c99e389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from keras.applications import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b320140d-b860-46ad-aa36-b9fe1e4c6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json file that contains the model's structure\n",
    "f = Path(\"model_structure.json\")\n",
    "model_structure = f.read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ae04ec8c-34c2-4692-b85d-9f6e24d54031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the Keras model object from the json data\n",
    "model = model_from_json(model_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "263c9953-7978-4f8a-8923-bfb81cb79965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model's trained weights\n",
    "model.load_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "063145ab-81c1-4bbd-abf6-af0405959368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image file to test, resizing it to 64x64 pixels (as required by this model)\n",
    "img = image.load_img(\"./05/dog.png\", target_size=(64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9a0e9e18-d25b-4f70-b00c-4eda6f5567d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to a numpy array\n",
    "image_array = image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c30b16b4-c16c-4f93-86d6-f1ad59a73411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a forth dimension to the image (since Keras expects a bunch of images, not a single image)\n",
    "images = np.expand_dims(image_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "20f547d5-26f0-46a3-9c73-693d6b0354c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "images = vgg16.preprocess_input(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "191beef1-7923-4fc3-8797-7c60aa889731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pre-trained neural network to extract features from our test image (the same way we did to train the model)\n",
    "feature_extraction_model = vgg16.VGG16(weights=\"imagenet\", include_top=False, input_shape=(64,64,3))\n",
    "features = feature_extraction_model.predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "111ae022-1d60-496e-b54b-b459df06a0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000156B996BD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# Given the extracted features, make a final prediction using our own model\n",
    "results = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "108396bc-a93c-446d-8949-34e99a9319ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are only testing one image with possible class, we only need to check the first result's first element\n",
    "single_result = results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "61fe9ffb-b799-4c45-9efa-6d7168f56418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood that this image contains a dog: 100%\n"
     ]
    }
   ],
   "source": [
    "# Print the result\n",
    "print(\"Likelihood that this image contains a dog: {}%\".format(int(single_result * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc184a-48e5-4ae9-901e-e243d7bc9b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb69df-5664-43e3-b7fb-7b4db4042502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6154cff-7ba3-4c93-ad8b-19f1e4a5fcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c765546-b0b2-45be-bc3c-c51c76129ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d7845-0aa2-497e-94e7-bc4798b38bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c266ba2-f868-4ad4-ab1c-378b1d4f0999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e2a30c-010f-4c8b-aa1a-12a017e59135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763dea2c-17f3-4358-a389-2e3931575ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334e300-e3c8-4073-8121-fb8a105cf1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a4417-a3ba-496a-8c14-4447e197e0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc1221-69ff-47df-b5fd-1435698412b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71159a8-e15d-4156-81cd-13de1e8bf4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
